<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>rpi-vr-camera WebXR/WebRTC Preview</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
      body {
        margin: 0;
        padding: 0;
        font-family: Arial, sans-serif;
        background: #111;
        color: #eee;
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 16px;
      }
      header {
        padding: 16px;
        text-align: center;
      }
      video {
        max-width: 90vw;
        background: #000;
      }
      .controls {
        display: flex;
        gap: 12px;
        align-items: center;
      }
      button {
        padding: 10px 16px;
        background: #1e88e5;
        border: none;
        color: #fff;
        border-radius: 4px;
        cursor: pointer;
      }
      button:disabled {
        background: #555;
        cursor: not-allowed;
      }
      pre {
        max-width: 90vw;
        white-space: pre-wrap;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>rpi-vr-camera WebRTC Preview</h1>
      <p>
        This page negotiates a WebRTC connection with the Raspberry Pi and
        displays the side-by-side stereo feed. For Oculus Quest, open this page
        via <code>https://&lt;pi-ip&gt;:8080</code> (you may need to accept a
        self-signed certificate or enable insecure origins).
      </p>
    </header>
    <div class="controls">
      <button id="startButton">Start Stream</button>
      <button id="stopButton" disabled>Stop</button>
      <button id="enterVrButton" disabled>Enter VR</button>
    </div>
    <video id="video" playsinline autoplay muted></video>
    <canvas id="xrCanvas" style="display:none"></canvas>
    <pre id="status"></pre>

    <script>
      const startButton = document.getElementById("startButton");
      const stopButton = document.getElementById("stopButton");
      const enterVrButton = document.getElementById("enterVrButton");
      const video = document.getElementById("video");
      const statusEl = document.getElementById("status");
      const xrCanvas = document.getElementById("xrCanvas");

      const hasWebXR = !!navigator.xr;

      let pc = null;
      let gl = null;
      let xrSession = null;
      let xrRefSpace = null;
      let videoTexture = null;
      let shaderProgram = null;
      let positionBuffer = null;
      let texcoordBuffer = null;
      let attribPosition = null;
      let attribTexCoord = null;
      let uniformTexOffset = null;
      let uniformTexScale = null;
      let uniformTexture = null;

      function log(message) {
        statusEl.textContent = message;
      }

      async function stopStream(message) {
        stopButton.disabled = true;
        startButton.disabled = false;
        enterVrButton.disabled = true;

        if (xrSession) {
          try {
            await xrSession.end();
          } catch (err) {
            console.warn("XR end error", err);
          }
          xrSession = null;
          xrRefSpace = null;
        }

        if (pc) {
          pc.getSenders().forEach((sender) => sender.track && sender.track.stop());
          await pc.close();
          pc = null;
        }

        video.srcObject = null;
        log(message || "Stream stopped.");
      }

      async function startStream() {
        startButton.disabled = true;
        stopButton.disabled = false;
        log("Starting WebRTC negotiationâ€¦");

        pc = new RTCPeerConnection({
          iceServers: [{ urls: ["stun:stun.l.google.com:19302"] }],
        });

        pc.addEventListener("track", (event) => {
          video.srcObject = event.streams[0];
          video.play().catch(() => {});
        });

        pc.addEventListener("connectionstatechange", () => {
          log(`Peer connection state: ${pc.connectionState}`);
          if (pc.connectionState === "failed" || pc.connectionState === "closed") {
            stopStream(`Connection ${pc.connectionState}`);
          }
        });

        pc.addTransceiver("video", { direction: "recvonly" });
        pc.addTransceiver("audio", { direction: "recvonly" });

        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const fail = async (message) => {
          await stopStream(message);
        };

        let response;
        try {
          response = await fetch("/offer", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify(pc.localDescription),
          });
        } catch (err) {
          await fail(`Offer network error: ${err}`);
          return;
        }

        const payload = await response.json().catch(() => ({}));
        if (!response.ok) {
          const message = payload.error || response.statusText;
          await fail(`Offer failed: ${message}`);
          return;
        }

        await pc.setRemoteDescription(payload);
        log("WebRTC connection established.");
        if (hasWebXR) {
          enterVrButton.disabled = false;
        }
      }

      function ensureGl() {
        if (gl) {
          return gl;
        }

        const contextAttributes = { xrCompatible: true, alpha: false };
        gl =
          xrCanvas.getContext("webgl2", contextAttributes) ||
          xrCanvas.getContext("webgl", contextAttributes);

        if (!gl) {
          log("WebGL not available; cannot enter VR.");
          return null;
        }

        const vertexSource = `
          attribute vec2 aPosition;
          attribute vec2 aTexCoord;
          varying vec2 vTexCoord;
          uniform vec2 uTexOffset;
          uniform vec2 uTexScale;
          void main() {
            gl_Position = vec4(aPosition, 0.0, 1.0);
            vTexCoord = uTexOffset + aTexCoord * uTexScale;
          }
        `;

        const fragmentSource = `
          precision mediump float;
          varying vec2 vTexCoord;
          uniform sampler2D uTexture;
          void main() {
            gl_FragColor = texture2D(uTexture, vTexCoord);
          }
        `;

        const compileShader = (type, source) => {
          const shader = gl.createShader(type);
          gl.shaderSource(shader, source);
          gl.compileShader(shader);
          if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
            const info = gl.getShaderInfoLog(shader);
            gl.deleteShader(shader);
            throw new Error(`Shader compile failed: ${info}`);
          }
          return shader;
        };

        const vertexShader = compileShader(gl.VERTEX_SHADER, vertexSource);
        const fragmentShader = compileShader(gl.FRAGMENT_SHADER, fragmentSource);

        shaderProgram = gl.createProgram();
        gl.attachShader(shaderProgram, vertexShader);
        gl.attachShader(shaderProgram, fragmentShader);
        gl.linkProgram(shaderProgram);
        if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {
          const info = gl.getProgramInfoLog(shaderProgram);
          throw new Error(`Program link failed: ${info}`);
        }
        gl.deleteShader(vertexShader);
        gl.deleteShader(fragmentShader);
        gl.useProgram(shaderProgram);

        positionBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.bufferData(
          gl.ARRAY_BUFFER,
          new Float32Array([
            -1, -1,
            1, -1,
            -1, 1,
            -1, 1,
            1, -1,
            1, 1,
          ]),
          gl.STATIC_DRAW
        );
        attribPosition = gl.getAttribLocation(shaderProgram, "aPosition");
        gl.enableVertexAttribArray(attribPosition);
        gl.vertexAttribPointer(attribPosition, 2, gl.FLOAT, false, 0, 0);

        texcoordBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, texcoordBuffer);
        gl.bufferData(
          gl.ARRAY_BUFFER,
          new Float32Array([
            0, 0,
            1, 0,
            0, 1,
            0, 1,
            1, 0,
            1, 1,
          ]),
          gl.STATIC_DRAW
        );
        attribTexCoord = gl.getAttribLocation(shaderProgram, "aTexCoord");
        gl.enableVertexAttribArray(attribTexCoord);
        gl.vertexAttribPointer(attribTexCoord, 2, gl.FLOAT, false, 0, 0);

        uniformTexOffset = gl.getUniformLocation(shaderProgram, "uTexOffset");
        uniformTexScale = gl.getUniformLocation(shaderProgram, "uTexScale");
        uniformTexture = gl.getUniformLocation(shaderProgram, "uTexture");
        gl.uniform1i(uniformTexture, 0);

        videoTexture = gl.createTexture();
        gl.bindTexture(gl.TEXTURE_2D, videoTexture);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGB, 2, 2, 0, gl.RGB, gl.UNSIGNED_BYTE, null);
        gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);

        gl.clearColor(0, 0, 0, 1);
        gl.disable(gl.DEPTH_TEST);
        gl.disable(gl.CULL_FACE);

        return gl;
      }

      async function enterVR() {
        if (!hasWebXR) {
          log("WebXR not supported in this browser.");
          return;
        }
        const supported = await navigator.xr.isSessionSupported("immersive-vr").catch(() => false);
        if (!supported) {
          log("Immersive VR not available on this device.");
          return;
        }
        const context = ensureGl();
        if (!context) {
          return;
        }
        if (xrSession) {
          await xrSession.end();
        }
        const session = await navigator.xr.requestSession("immersive-vr", {
          optionalFeatures: ["local-floor", "bounded-floor"],
        });
        xrSession = session;

        if (context.makeXRCompatible) {
          await context.makeXRCompatible();
        }
        const glLayer = new XRWebGLLayer(xrSession, context);
        xrSession.updateRenderState({ baseLayer: glLayer });
        xrRefSpace = await xrSession.requestReferenceSpace("local");

        xrSession.addEventListener("end", () => {
          xrSession = null;
          xrRefSpace = null;
          log("VR session ended.");
        });

        xrSession.requestAnimationFrame(onXRFrame);
        log("Entered VR session.");
      }

      function onXRFrame(time, frame) {
        if (!xrSession || !gl) {
          return;
        }
        xrSession.requestAnimationFrame(onXRFrame);
        const pose = frame.getViewerPose(xrRefSpace);
        if (!pose) {
          return;
        }

        const layer = xrSession.renderState.baseLayer;
        gl.bindFramebuffer(gl.FRAMEBUFFER, layer.framebuffer);

        if (video.readyState >= video.HAVE_CURRENT_DATA) {
          gl.bindTexture(gl.TEXTURE_2D, videoTexture);
          gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGB, gl.RGB, gl.UNSIGNED_BYTE, video);
        }

        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.useProgram(shaderProgram);

        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.vertexAttribPointer(attribPosition, 2, gl.FLOAT, false, 0, 0);
        gl.bindBuffer(gl.ARRAY_BUFFER, texcoordBuffer);
        gl.vertexAttribPointer(attribTexCoord, 2, gl.FLOAT, false, 0, 0);

        for (const view of pose.views) {
          const viewport = layer.getViewport(view);
          gl.viewport(viewport.x, viewport.y, viewport.width, viewport.height);
          const isLeft = view.eye === "left";
          gl.uniform2f(uniformTexOffset, isLeft ? 0.0 : 0.5, 0.0);
          gl.uniform2f(uniformTexScale, 0.5, 1.0);
          gl.drawArrays(gl.TRIANGLES, 0, 6);
        }

        gl.bindFramebuffer(gl.FRAMEBUFFER, null);
      }

      startButton.addEventListener("click", startStream);
      stopButton.addEventListener("click", () => stopStream());
      enterVrButton.addEventListener("click", () =>
        enterVR().catch((err) => stopStream(`XR error: ${err}`))
      );
    </script>
  </body>
</html>
